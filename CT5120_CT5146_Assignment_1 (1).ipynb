{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT5120_CT5146_Assignment_1_(3)_(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUER4RlLeHHG"
      },
      "source": [
        "*Name* : Saikrishna Javvadi \n",
        "\n",
        "*Student_ID* : 20236648 \n",
        "\n",
        "*Course* : MSc in CS - Data Analytics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oP1uun77cIh"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIVCSJV-7kDs"
      },
      "source": [
        "## Task 1 (10 Marks)\n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RznCZ1mw7mfk",
        "outputId": "06cda523-6ad3-4896-c18f-16aff3580d67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Reading the txt file from drive\n",
        "lines = open('/content/drive/My Drive/holbrook.txt').readlines()\n",
        "\n",
        "clean_lines = [] \n",
        "# store the lines in this list after removing the leading and trailing spaces\n",
        "for line in lines:\n",
        "  clean_lines.append(line.strip())\n",
        "    \n",
        "# Word tokenizing the sentences\n",
        "sentences = [nltk.word_tokenize(sentence) for sentence in clean_lines]\n",
        "\n",
        "data = []\n",
        "def data_prep(sentence):  \n",
        "    original = []\n",
        "    corrected = []\n",
        "    indexes = []\n",
        "    count = 0\n",
        "    \n",
        "    for i in sentence:\n",
        "        if '|' in i:\n",
        "            # Splitting the sentence based on '|'\n",
        "            word = i.split('|')\n",
        "            # word that is previous to '|' is stored in original list.\n",
        "            original.append(word[0])\n",
        "            #word that is next to '|' is storied in corrected list.\n",
        "            corrected.append(word[1])\n",
        "            #Index that the error occured at\n",
        "            indexes.append(count)\n",
        "        \n",
        "        else:\n",
        "            # If there is no '|' in sentence, sentence is stored in original and corrected as it is.\n",
        "            original.append(i)\n",
        "            corrected.append(i)\n",
        "        count = count+1\n",
        "        \n",
        "    #Loading to original, corrected and indexes list to dictionary.      \n",
        "    dictionary = {'original': original, 'corrected': corrected, 'indexes': indexes}\n",
        "    \n",
        "    return dictionary\n",
        "\n",
        "for sentence in sentences:\n",
        "  data.append(data_prep(sentence))\n",
        "    \n",
        "#print(data[2])\n",
        "assert(data[2] == {\n",
        "    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
        "    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
        "    'indexes': [9]\n",
        "})"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRSX4I0H7pSC"
      },
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt9aR2Gy7p1C",
        "outputId": "869d5aa4-6c7f-4c91-8407-be143717021f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = data[:100]\n",
        "train = data[100:]\n",
        "print(len(train))\n",
        "print(len(test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1117\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5JL7cH7sLK"
      },
      "source": [
        "## **Task 2** (10 Marks): \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ge0uHS-7uEK",
        "outputId": "01e1e683-aba7-45fb-d71a-8a7d0022704f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#The below implementations are referred to from lab exercise sheets\n",
        "def unigram(given_word):\n",
        "    count = 0\n",
        "    for data in train: \n",
        "      for word in data['corrected']:\n",
        "        word = word.lower()\n",
        "        if word == given_word:\n",
        "            count += 1\n",
        "    return count\n",
        "    \n",
        "def prob(given_word):\n",
        "    count = 0\n",
        "    total = 0\n",
        "    for data in train: \n",
        "      for word in data['corrected']:\n",
        "        word = word.lower()\n",
        "        if word == given_word:\n",
        "            count += 1\n",
        "        total += 1  \n",
        "       \n",
        "    return count/total\n",
        "    \n",
        "print(prob(\"me\"))\n",
        "assert(unigram(\"me\")==87)\n",
        "\n",
        "#Alternate implementation using Counter class\n",
        "from collections import Counter\n",
        "def unigram(given_word):\n",
        "    #creating a list of all the words available in corrected training dataset\n",
        "    words = [given_word.lower() for data in train for given_word in data['corrected']] \n",
        "    #The counter class from collections helps us to create a mapping of each word with it's respective count\n",
        "    count = Counter(words)  \n",
        "    #Returning count of given_word  from the dictionary of mappings above                                                  \n",
        "    return count[given_word]                                                     \n",
        "\n",
        "def prob(given_word):\n",
        "    #creating a list of all the words available in corrected training dataset\n",
        "    words = [given_word for data in train for given_word in data['corrected']] \n",
        "    #returning probability of a given_word which is the number of occurences of the word to total number of words\n",
        "    return unigram(given_word)/len(words)                         \n",
        "print(prob(\"me\"))\n",
        "assert(unigram(\"me\")==87)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.003989361702127659\n",
            "0.003989361702127659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8r8QYj78GPK"
      },
      "source": [
        "## **Task 3** (15 Marks): \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9Mu8P38IQE",
        "outputId": "405a746b-5e70-4d55-80d0-daea17deef17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm46Lbiz8K8M"
      },
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoilAmFW8PCb",
        "outputId": "2dc8a21e-1153-4e91-b5a9-c1d767fa6e82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import collections\n",
        "def get_candidates(token):\n",
        "    #list of all unique tokens in train\n",
        "    unique_words = list(set(word for data in train for word in data['corrected']))\n",
        "    \n",
        "    # Storing the nearest words for each word in a dictionary\n",
        "    distance = collections.defaultdict(int) \n",
        "    for word in unique_words:\n",
        "        # Calculating distance between the word and token\n",
        "        distance[word] = edit_distance(word,token)\n",
        "    #sorting the dictionary based on edit_distance\n",
        "    sorted_distance = dict(sorted(distance.items(), key=lambda x:x[1]))\n",
        "    #Finding the minimum edit distance possible between given token and the word from train\n",
        "    minimal_distance = list(sorted_distance.values())[0]\n",
        "    \n",
        "    #list of candidate words from training set which have the lowest edit_distance from the given token\n",
        "    candidates = list(filter(lambda k: sorted_distance[k] == minimal_distance, sorted_distance.keys()))\n",
        "    \n",
        "    return candidates\n",
        "  \n",
        "print(get_candidates(\"minde\"))   \n",
        "assert(get_candidates(\"minde\") == ['mine', 'mind'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mine', 'mind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGY-eCkN8TIM"
      },
      "source": [
        "## Task 4 (15 Marks):\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIGKE4_P8WGP",
        "outputId": "27420750-3d88-4563-e014-9028ffeaf2c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def correct(sentence):\n",
        "    unique_words = list(set(word for data in train for word in data['corrected']))\n",
        "    corrected = []\n",
        "    count = 0\n",
        "    indexes = []\n",
        "    for word in sentence:\n",
        "        # If a given word is not in unique_words then calculate suggested_words with minimal distance\n",
        "        if word.lower() not in unique_words:\n",
        "            indexes.append(count)\n",
        "            if len(get_candidates(word)) > 1:\n",
        "                suggested_words = get_candidates(word)\n",
        "                problist = []\n",
        "            # Calculating the unigram probability for each suggested word\n",
        "                for suggestion in suggested_words:\n",
        "                  p1 = prob(suggestion)\n",
        "                  problist.append(p1)     \n",
        "                if len(suggested_words[problist.index(max(problist))]) > 1:\n",
        "                    corrected.append(suggested_words[problist.index(max(problist))])\n",
        "                else:\n",
        "                    corrected.append(suggested_words[problist.index(max(problist))])\n",
        "            # If there is only one suggested word appending the word to list without having to calculate the probablities\n",
        "            else:\n",
        "                corrected.append(get_candidates(word)[0])\n",
        "        else:\n",
        "            corrected.append(word)\n",
        "        count = count+1\n",
        "  \n",
        "    return corrected\n",
        "\n",
        "\n",
        "print(correct([\"this\",\"whitr\",\"cat\"]))\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'white', 'cat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG7jC6au8kka"
      },
      "source": [
        "## **Task 5** (10 Marks): \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSXTQypR8mdR",
        "outputId": "3c372ea3-6baf-4856-ec51-bac428544154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def accuracy(actual_sentence, sentence_to_be_modified):\n",
        "    actual = actual_sentence\n",
        "    corrected = correct(sentence_to_be_modified)\n",
        "    # Dividing the length of matching words  between actual and corrected sentence  by length of actual sentence to calculate accuracy \n",
        "    #print(set(corrected))\n",
        "    #print(set(corrected))\n",
        "    #print(set(actual)& set(actual))\n",
        "    accuracy = len(set(corrected) & set(actual))/len(set(actual))\n",
        "    return accuracy\n",
        "\n",
        "acc = []\n",
        "for i in range(len(test)):\n",
        "    acc.append(accuracy(test[i]['corrected'], test[i]['original']))\n",
        "    \n",
        "print(\"Prediction Accuracy:\", sum(acc)/len(acc))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction Accuracy: 0.8140010650970398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-r2JzD8_Zh"
      },
      "source": [
        "## **Task 6 (35 Marks):**\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3L1xqqVfmva"
      },
      "source": [
        "**Modifications to the above approach:**\n",
        "\n",
        "I  am using wordnet to check if a given word is valid instead of validating it with just the training data(WordNet is a lexical database for the English language which is part of the NLTK corpus). That is, I am implementing lemmatization and stemming to get the base word for each word in the sentence and check if the new lemmatized/stemmed word is present in  wordnet.words(). This was implemented to recognise words that appear in different forms but which might be present in the wordnet but not available in training data.\n",
        "\n",
        "We can observe an increase the accuracy of the model by ~8% by after the implementation of the above step\n",
        "\n",
        "We could also implement smoothing techniques to avoid zero probability and a bigram/trigram probability language model could also increase the performance of the model. But, since most of the sentences in the corpus are small, I don't think they would significantly increse the performance for this model. I assume collecting more training data will increase the performance of the model significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6CRxU2Pfvj8",
        "outputId": "5248dd42-a957-4771-aedc-638faf402b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def modified_correct(sentence):\n",
        "    corrected = []\n",
        "    count = 0\n",
        "    indexes = []\n",
        "    unique_words = list(set(word for data in train for word in data['corrected']))\n",
        "   \n",
        "    for word in sentence:\n",
        "        \n",
        "        # If word not in unique word the calculate suggested words with minimal distance\n",
        "        # Also checking if the lemmatized and stemmed words are present in using words.words() for word validation.\n",
        "        if word.lower() not in unique_words  and lemmatizer.lemmatize(word.lower()) not in wn.words() and word.lower() not in wn.words()  and stemmer.stem(word.lower()) not in wn.words():\n",
        "            indexes.append(count)\n",
        "            if len(get_candidates(word)) > 1:\n",
        "                suggested_words = get_candidates(word)\n",
        "\n",
        "                problist = []\n",
        "            # Calculating the unigram probability for each suggested word\n",
        "                for suggestion in suggested_words:\n",
        "                  p1 = prob(suggestion)\n",
        "                  problist.append(p1)     \n",
        "                if len(suggested_words[problist.index(max(problist))]) > 1:\n",
        "                    corrected.append(suggested_words[problist.index(max(problist))])\n",
        "                else:\n",
        "                    corrected.append(suggested_words[problist.index(max(problist))])\n",
        "            # If there is only one suggested word appending the word to list without calculating probablities\n",
        "            else:\n",
        "                corrected.append(get_candidates(word)[0])\n",
        "        else:\n",
        "            corrected.append(word)\n",
        "        count = count+1\n",
        "    #Returning the corrected sentence\n",
        "    return corrected\n",
        "\n",
        "print(modified_correct([\"this\",\"whitr\",\"cat\"]))\n",
        "assert(modified_correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['this', 'white', 'cat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLzaC6D28sK9"
      },
      "source": [
        "## **Task 7 (5 Marks):**\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw6PzwWn7iEo",
        "outputId": "6469aaae-d224-4765-ea16-ac29e9040ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def accuracy(actual_sentence, sentence_to_be_modified):\n",
        "    actual = actual_sentence\n",
        "    corrected = modified_correct(sentence_to_be_modified)\n",
        "    # Dividing the length of matching words  between actual and corrected sentence  by length of actual sentence to calculate accuracy \n",
        "    accuracy = len(set(corrected) & set(actual))/len(set(actual))\n",
        "    return accuracy\n",
        "\n",
        "acc = []\n",
        "for i in range(len(test)):\n",
        "    acc.append(accuracy(test[i]['corrected'], test[i]['original']))\n",
        "    \n",
        "print(\"Prediction Accuracy:\", sum(acc)/len(acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction Accuracy: 0.8915912720034233\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}